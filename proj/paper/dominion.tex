
% modeling Dominion in Hakaru, and what kinds of questions are interesting
% to ask the model
\section{Dominion} \label{sec:dom}

Dominion is a game we are familiar with. This game falls into the category of
{\bf turn-based
deckbuildings game}, where a player \emph{builds} a good deck by choosing what
cards to buy on their turn. The probabilistic component of deckbuilding games
arises from shuffling decks of cards at certain points in the game. In particular
in Dominion the cards a player plays can affect both when cards get shuffled
and the composition of the deck being shuffled. As a result, game-play heuristics
for Dominion are inherently intertwined with the probabilistic game state.

\subsection{Game State}

\subsection{Heuristics} \label{sec:dom:heuristics}
We

% -----------------------------------------------------------------------------
% Theory of the greedy-vs-greedy experiment
\subsection{Greedy Theory}
A simple query one might wish to ask of the greedy heuristic is:

\begin{quote} \label{quote:dominion-query}
Given that we buy card $X$ with probability $p_0$ and card $Y$ with
probability $(1 - p_0)$, what is the distribution over the length of
the game?
\end{quote}

In this query, we have model parameters comprising:

\begin{itemize}
\item $X$  \hsk{:: Card}
\item $Y$  \hsk{:: Card}
\item $p_0$ \hsk{:: Probability}
\end{itemize}

We also have implicit hyper-parameters comprising (see Appendix
\ref{appendix:runtime-cards} for an overview of the Haskell
implementation):

\begin{itemize}
\item \hsk{kingdomCards :: [Card]}
\item \hsk{turnRules    :: [GameParameters]}
\end{itemize}



One observe-query inference question to ask the model then is:

\begin{equation} \label{q:inference}
...
\end{equation}

% -----------------------------------------------------------------------------
% Results figures:
\subsection{Greedy Results \& Analysis} \label{sec:dom:greedy-results}
We now present results for the greedy heuristic, as described in
Section \ref{sec:dom:heuristics}. Before answering the inference
question (Question \ref{q:inference}), we first take unconditioned
samples from the greedy-vs-greedy model to determine characteristic
game lengths. The result of this sampling is realized as a
gaussian-like distribution of Figure \ref{fig:turn-dist}. Sampling
the model with other values of the \hsk{VILLAGE} - \hsk{CHANCELLOR}
buy ratio $p_0$ shows that $p_0$ is inversely related to the mean
of the gaussian $\mu_{t}$.

From this information we infer that the \hsk{VILLAGE} card has
a greater impact on how quickly the game ends. This is interesting
to a player of Dominion attempting to come up with a ranking of
cards. Performing an automated analysis on every possible pair
of cards using this sampling technique, one could:

\begin{itemize}
\item Design a good greedy buy heuristics
\item Determine the balance of the hyper-parameter corresponding
      to which cards are being used in the game.
\end{itemize}

The \hsk{VILLAGE} having a greater impact on the game ending is
also in line with the personal belief of the author that a
\hsk{VILLAGE} is, in general, more useful than a \hsk{CHANCELLOR}.
However, what the gaussian form of Figure \ref{fig:turn-dist}
does not tell us is the characteristic distribution of values of
$p_0$ for a given game length. This is what the inference Question
\#\ref{q:inference} is trying to give us insight into.

%While this is
%interesting and is in line with the personal belief of the author
%that a \hsk{VILLAGE} is generally more useful than a
%\hsk{CHANCELLOR}, it does not say anything about the characteristic
%value of $p_0$ for a given game length.

%TODO:
%figure: one column showing the un-normalized (frequency) plots of
%the rejection sampling, second column showing the normalized
%(probability density) 
%OR: one column of 3 plots, with the left-hand axis corresponding to
%frequency and the right-hand y-axis corresponding to probability.
%plot the distributions / barcharts on the same graph in different
%colors / with some alpha value.

In Figure \ref{fig:rejection-sampling} we see the characteristic
form of the turn-conditioned distribution over the
\hsk{VILLAGE} - \hsk{CHANCELLOR} buy ratio ($p_0$). At first glance
these distributions appear to be linear in $p_0$. In terms of our
parameters we can therefore write the equation:

\begin{equation} \label{eqn:linear-distribution}
P(p_0 | t_e = t_e') = m(p_0, X, Y) * p_0 + b(p_0, X, Y)
\end{equation}

In this equation we now have a name for the observed random variable
`game length' ($t_e$, the end turn number).
We also have $t_e'$ a specific (conditional) value for $t_e$.
Finally the latent random variables $m$ and $b$ together comprise a
characteristic description of the conditional distribution. The
key to this equation is that $m$ and $b$ are variables which cannot
be directly sampled / observed and have a probabilistic causal affect
on the observed variable $t_e$.
Since we have no feasible means for determining the closed analytical
forms of $m$ and $b$ we have relied on the Markov Chain Monte-Carlo (MCMC)
methods as described earlier in Section \ref{sec:mcmc}.

For the specific case of $X =$ \hsk{VILLAGE} and $Y =$ \hsk{CHANCELLOR},
we see (Figure \ref{fig:rejection-sampling}) a relationship where as the
number of turns $t_e'$ we condition on goes down...

\begin{itemize}
\item the slope $m$ of the conditional distribution increases from negative to positive
\item the intercept $b$ decreases
\item the area under the distribution gradually shifts from low values of $p_0$ to
      high values of $p_0$.
\end{itemize}

These trends are unsurprisingly in accordance with the trend of $\mu_{t_e}$,
the mean of the gaussian distribution of Figure \ref{fig:turn-dist}. Namely
that the \hsk{VILLAGE} to \hsk{CHANCELLOR} ratio increases as the number of
turns in the game decreases. The new insight we have gained from our MCMC
methods is the form of Equation \ref{eqn:linear-distribution} as well as values
for the latent variables $m$ and $b$ at various conditioned points. The form
of the conditional distribution is in fact itself a latent variable which
can be dependent on both the hyper-parameters to the game engine and parameters
to the probabilistic model. Presently it is a latent variable which can only be
computed by a human, but given a good fitting algorithm could be automated.
See Section \ref{sec:future} for further discussion of analysis automation.

% TODO: this paragraph...
variables of $t_e$ the observed game
length random variable, $t_e'$ a specific value for $t_e$, and
$m$ and $b$ the latent random variables which together comprise
a characteristic description of the conditional distribution
as described on the right-hand side of the equation.

% -----------------------------------------------------------------------------
% Results figures:

\begin{figure}
\includegraphics[width=.95\columnwidth]{../pres/village-chancellor-turn-dist.png}
\caption{\label{fig:turn-dist} Probability distribution over number of turns
in a 2-player game of Dominion.
Both players play a greedy strategy with model
parameters of $X = \textrm{Village}$, $Y = \textrm{Chancellor}$, and
$p_0 = 0.5$. See Appendix \ref{app:dominion-card} for a complete description
of the semantics behind these cards and why this pair of cards is interesting
for the game Dominion.
This distributions is unconditioned, therefore only requiring sampling directly
from the model (no inference).
}\end{figure}


% future work: less noisy / more samples / error bars on the bar charts
%               -- validate the form of the distribution statistically

\section{Probabilistic Language Analysis} \label{sec:meta-analysis}

% Need some sort of combinator library / monadic abstraction which
% keeps track of the level of abstraction you want to describe your
% model at. "Hyper-parameter" is an ambiguous name for something
% which your probabilistic model does not condition on / 'simply'
% indirectly changes the structure of the probabilistic computation
% -- for a probabilistic model to be modular in a way that e.g. both
% -- game designers and game players can use, the probabilistic
% -- abstractions need to encompass some sort of naming scheme /
% -- distinction between the level of abstraction at which a parameter
% -- resides. How domain-specific are these abstractions? How universal
% -- is the distinction between e.g. a hyper-parameter which changes
% -- the fundamental structure of an expression in the condition of
% -- an if-statement as compared to a model hyper-parameter which
% -- changes the value of a random variable we are conditioning on.
%    Clearly there are both linguistic and probabilistic implications of
%    of how we classify these parameters.
Existing probabilistic languages seem to rely heavily on the linguistic
constructs given to them by their host languages. From our experience and
current discussion of BLOG and Hakaru, we see that PPLs
which tend to defer to their host language's abstraction capabilities are more
expressive and powerful. At the same time such languages also rely
heavily on the model of computation given to them by their host language.
Hakaru relies heavily on Haskell's monadic mode of computation. BLOG on
the other hand has developed the {\bf many-worlds} model of computation.

Probabilistically, the latter model of computation can be very satisfying
in terms of its similarity to the mathematical description of a probabilistic
process. In many situations declaratively
describing a probabilistic model of the possible worlds is more natural.
In the domain of games, one can envision a declarative enumeration
of the rules in a game and how they affect the game state.

Such rules form the underlying mechanics of a game. The probabilistic
components of these mechanics are, as discussed throughout this work,
implementable in PPLs. The difficulties in creating these implementations
in Hakaru in particular are described below (Section \ref{sec:hakaru-analysis}).

\section{Hakaru Analysis} \label{sec:hakaru-analysis}

When implementing in Haskell a complex model like that of Dominion, a
need for various monadic structures arises. In the case of Dominion
this includes the \hsk{State} and \hsk{Measure} monads (and eventually
the \hsk{IO} monad). It would therefore have been helpful for Hakaru's
\hsk{Measure} monad to support standard Haskell monads (\hsk{State} in
particular). The current implementation of our model manages to combine
these two monads using the \hsk{StateT} monad transformer and the use
of Haskell's default \hsk{lift} to move values between the \hsk{State} and
\hsk{Measure} monads. This model and more complex models using other
monads would certainly benefit from Hakaru having its \hsk{Measure}
monad derive more typeclasses.

Similarly Hakaru's interface into Haskell's random number support was
unsatisfying to use. In particular the modeler cannot specify a source
of randomness at the top-level operations Hakaru exposes. While the
default source of randomness Hakaru uses is applicable for many situations,
it makes meta-programming difficult. In the implementation of our model
we needed to re-write some of the sampling interfaces to consume a parametric
source of randomness rather than a hard-coded one in order to get the
desired effect.

Another design flaw currently baked into Hakaru's runtime system is the
use of runtime errors to unconditionally terminate the program when
Hakaru sees something it is unable to handle. The existence of
one of these errors is likely indicative of a bug in the model supplied
to Hakaru. However it is unsatisfying because it both inhibits
meta-programming and debugging.

Meta-programming is inhibited because a modeler may for instance want to
programmatically test for the correctness of a model by seeing if it
produces a runtime error. This is only feasible if the runtime error
is in some way `catchable'. Hakaru could for instance use Haskell's
\hsk{State} monad to maintain both a list of warnings and errors
along with the contents of its \hsk{Measure} monad.

Similarly runtime errors inhibit future Hakaru-specific support for
debugging because it relies on a decidedly imperative feature in a
functional host language. In particular one can modularize the implementation
of model debugging support in Hakaru if one takes advantage of the full
expressive power of Haskell in lieu of relying solely on Haskell's \hsk{error}
semantics. It would also be very helpful for Hakaru to print user-friendly
error messages and suggestions about common mistakes seen in probabilistic
models.

\section{BLOG Analysis} \label{sec:blog-analysis}

In addition to Hakaru we also look at the feasibility of defining a
probabilistic model of Dominion as a BLOG model. The general form
of Hakaru's issues just described in Section \ref{sec:hakaru-analysis}
is `we have a powerful host language, the abstractions
just need to be refined.' The form of BLOG's issues we discuss in this
section is `we have a powerful model of computation and a set of
natural probabilistic abstractions, but the linguistic components
of BLOG are presently undeveloped or poorly abstracted.'

One such poorly designed abstraction is the use of comprehensions
in BLOG's grammar. BLOG currently supports set-comprehensions
but not list-comprehensions. Grammatically (and syntactically) these
two features are virtually identical.
It is evident from this that BLOG's grammar has done a poor job of
distinguishing between general purpose linguistic features and
probabilistic-specific features. It is this author's opinion that
there should be a clean distinction between probabilistic and
general purpose features. BLOG in particular is, to some extent, embedded
in Java. This embedding could be exploited to a similar extent that
Hakaru exploits features of Haskell. Such an embedding would forgo the
need to `reinvent the wheel' in terms of making BLOG a more expressive
language.

The current trajectory of BLOG appears to be towards tailoring
general purpose language features to BLOG's particular model of computation.
The ambitious nature of this goal could eventually give BLOG a highly
expressive modeling form in conjunction with an efficient model of
computation. Presently however, BLOG falls significantly short of the
expressive power needed to naturally model significantly complex
probabilistic processes.


